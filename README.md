# EMMA-X
## Training
To train EMMA-X, run:
```
python train.py
```
## XRETE Benchmark
To use XRETE benchmark, you can download each task using the following links:
1. [AmericasNLI](https://github.com/abteen/americasnli)
2. [BUCC2018](https://github.com/google-research/xtreme/blob/master/scripts/run_bucc2018.sh)
3. [LaReQA](https://github.com/google-research-datasets/lareqa)
4. [Mewsli-X](https://github.com/google-research/xtreme/blob/master/scripts/download_mewslix_data.sh)
5. [Amazon Review Corpus](https://docs.opendata.aws/amazon-reviews-ml/readme.html)
6. [MultiEURLEX](https://huggingface.co/datasets/multi_eurlex)
7. [MultiSTS](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/STS2017-extended.zip)
8. [PAWS-X](https://github.com/google-research/xtreme/blob/master/scripts/train_pawsx.sh)
9. [Tatoeba](https://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1)
10. [WMT21-Quality Evaluation](https://www.statmt.org/wmt21/quality-estimation-task.html)
11. [XCOPA](https://github.com/google-research/xtreme/blob/master/scripts/train_xcopa.sh)
12. [XNLI](https://github.com/google-research/xtreme/blob/master/scripts/train_xnli.sh)

For BUCC2018, LaReQA, Mewsli-X, PAWS-X, XCOPA, and XNLI, we evaluate follow the scripts in [XTREME](https://github.com/google-research/xtreme/tree/master). For other tasks, we use the official evaluation scripts used in their papers.

For geometric evaluation, the dataset is provided in [FLORES-200](https://tinyurl.com/flores200dataset)
